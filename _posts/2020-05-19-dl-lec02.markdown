---
layout:     post
title:      모두의 딥러닝 - Lecture 02 정리
author:     Soo-young Hwang
tags: 		Deep-Learning
subtitle:  	Lec02 - Linear Regression
category:  study
---

### 서론
[모두의 딥러닝](https://hunkim.github.io/ml/) Lecture 02 을 듣고 추가로 공부를 하면서  정리하였습니다.    
Lectur 02의 주제는 **Linear Regression** 입니다.   
    
### 본론

#### 강의 요약

- Linear Regression이란?
- Linear Regression의 cost function(lost function)

#### 내용

<blockquote>Linear Regression</blockquote>    

`Regression` 은 **Supervised Learning**의 하나로 Training Set으로 학습을 한다.   

![1](https://swimmingHwang.github.io/img/dllec02-1.png)   

강의에서 간단한 예시로 설명을 한다.   
위의 이미지에서 Table의 데이터를 학습(Train)시켜 모델을 생성한다면, `x`에 7을 넣으면 `65`라고 예측한다.   

세상에 있는 많은 현상이 Linear한 형태를 가진다.   
예를 들어, 훈련을 하는 시간이 많다면 좋은 성과를 낸다.  ..등   

![2](https://swimmingHwang.github.io/img/dllec02-2.png)    
`Regression` 에서 `H(x) = Wx + b`, 일차식 가설을 세우고 뭐가 더 좋은지 고르는 과정이 필요하다.   

![3](https://swimmingHwang.github.io/img/dllec02-3.png)   
`Cost function`(lost function) 은 가설과 실제 데이터가 얼마나 다른가를 나타내준다.  
`Linear Regression` 에서는 거리를 통해 구한다.   
![4](https://swimmingHwang.github.io/img/dllec02-4.png)    
`Linear Regression` 의 `cost function` 구하는 과정이다.   
![5](https://swimmingHwang.github.io/img/dllec02-5.png)    
`Linear Regression` 모델의 가설에 대한 목표는 `cost`를 최소화 하는 것 이다.

### 결론
다음은 Linear Regression 실습을 다뤄보겠습니다.


너무 오랜만에 머신러닝 공부를 했다...!
좀더 자주 해야지~~!!!!


### References
[참고 사이트 1 https://hunkim.github.io/ml/](https://hunkim.github.io/ml/)   
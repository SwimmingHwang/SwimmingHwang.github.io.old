---
layout:     post
title:      모두의 딥러닝 - Lecture 03 정리
author:     Soo-young Hwang
tags: 		Deep-Learning
subtitle:  	Lec03 - How to minimize cost
category:  study
---

### 서론
[모두의 딥러닝](https://hunkim.github.io/ml/) Lecture 03 을 듣고 추가로 공부를 하면서  정리하였습니다.    
Lectur 03의 주제는 **How to minimize cost** 입니다.   
    
### 본론

[지난 포스트](http://swimminghwang.github.io/study/2020/05/19/dl-lec02/)에서 `Linear Regression`이 뭔지, 그리고 `Linear Regression`의 `cost function`을 배웠습니다.   
Cost function은 가설과 실제 데이터가 얼마나 다른가를 나타냅니다.   
Linear Regression의 목표는 **`Cost function`이 최소가 되는 W,b의 값을 구하는 것** 입니다.   
이번 포스트에선 **Cost를 최소화하는 방법에 대한 개요**를 다뤘습니다.   



<blockquote>How to minimize cost?</blockquote>    

`Linear Regression` 의 Hypothesis는    
<p><execode>H(x) = Wx + b</execode></p>
b가 0이라고 했을 때, Cost 함수는   

![2](https://swimmingHwang.github.io/img/dllec03-2.png)   
이다. 

`W`에 1, 0, 2 를 대입해 본 결과 아래 그래프처럼 그릴 수 있다.   

![3](https://swimmingHwang.github.io/img/dllec03-3.png) 

이 식에 더 많은 W의 값을 대입해 보면, 

![1](https://swimmingHwang.github.io/img/dllec03-1.png)   

이 결과를 얻을 수 있다.   
앞에 말했듯이 `Linear Regression`의 목표는 **Cost가 최소가 되는 W,b** 를 찾는 것이다.   
여기선 b를 0이라고 했으니, W만 찾으면 된다.   
사람이 봤을 땐, 바로 W가 1인 지점이 Cost 가 제일 작다는 것을 알 수 있다.   
하지만 컴퓨터는 아니다.   
그래서 필요한 것이 `Grdient descent algorithm`이다. 

<blockquote>Gradient descent algorithm</blockquote>

Gradient : 경사   
descent : 내려감  
경사를 따라 내려가는 알고리즘   
한국말로 **경사하강법** 이라고 불리운다.   

이 방법의 원리는 산 정상에서 골짜기로 가장 빠르게 내려오는 방법인 가장 경사가 가파른 길을 골라 내려오는 것과 같은,   
**최적화 알고리즘** 중 하나이다.   

**W가 어디서 시작하든 최종적으로 cost가 0인 1에 도달해야 한다.**   

이 알고리즘은 어디서 출발하는 최저점에 도달하는게 장점이다.   
W의 값에 조금씩 변화를 주면서 cost를 줄여나간다.   
이 과정을 반복하여 최저점에 도달하는 과정으로 작동한다.   

경사도를 사용하기에 미분이 필요하다.   
공식의 정의는 아래와 같다.   

![4](https://swimmingHwang.github.io/img/dllec03-4.png)   
![5](https://swimmingHwang.github.io/img/dllec03-5.png)   

`Gradient Desecent Algorithm`을 거쳐 `Linear Regression`은 Cost가 최소가 되는 W,b 의 값을 얻을 수 있다.   

경사하강법, Gradient Desecent Algorithm은 볼록한 모양을 가진 함수에서 반드시 Optimal한 값이 하나 밖에 존재하지 않는다.   
이러한 모양을 가진 함수를 `Convex Function`이라고 한다.   

<blockquote>Convex Function</blockquote>

`Convex Function`은 밥그릇처럼 생긴 함수이다.   
항상 답을 찾을 수 있다는 보장을 해 주며, 
`Linear Regression`을 할시 `Convex Function`인지 확인해야 한다.   

![6](https://swimmingHwang.github.io/img/dllec03-6.png)   

더 자세한 설명은 [이 링크](http://sanghyukchun.github.io/63/)를 참고하면 된다.   


### 결론
학부때 경사하강법을 배웠는데 개념만 알았기에 어떤 원리인지 몰랐었다.   
이제야 원리를 알게 되어서 불행중 다행이다.^,^   

Convex Function에 대해 더 자세히 봐도 좋을 것 같다.   
지금은 잘 이해를 못하겠다...ㅎㅎ   

### References
[참고 사이트 1 https://hunkim.github.io/ml/](https://hunkim.github.io/ml/)   